# RAG Project: Retrieval-Augmented Generation with LoRA

## üöÄ Overview

This project demonstrates a **Retrieval-Augmented Generation (RAG)** pipeline using:

- **Python FastAPI** embedding service
- **Node.js/Express** orchestrator
- **MongoDB Atlas** vector store
- **LoRA adapters** for lightweight fine-tuning of transformer models
- **Hugging Face Transformers + PEFT** for adapter training and inference

The system allows you to ingest documents, perform context-aware queries, and apply LoRA adapters for efficient model customization.

---

## üèó Architecture

                         +-------------------+
                         |                   |
                         |  Node.js          |
                         |  Orchestrator     |
                         |  (API / LLM)      |
                         |                   |
                         +---------‚áÑ---------+
                                   |
                                   | 
                                   v
                         +-------------------+
                         |                   |
                         |  Python Embedding |
                         |  Service (API)    |
                         |                   |
                         +---------‚áÑ---------+
                                   |
                                   | Stores / retrieves embeddings
                                   v
                         +-------------------+
                         |                   |
                         | MongoDB Atlas     |
                         | Vector Store      |
                         |                   |
                         +-------------------+

‚úÖ Explanation of double arrows (‚áÑ):

Between Node orchestrator and Python service: query sent ‚Üí embedding/context returned.

Between Python service and MongoDB: embedding upsert & retrieval are bidirectional.


---

## üì¶ Running Locally

### 1. Clone repository

```bash
git clone https://github.com/surbhi498/RAG_ASSESMENT_ARCHITECTURE.git
cd rag_assesment

## Create virtual environment for Python3
python3 -m venv .venv1
source .venv1/bin/activate
pip install -r requirements1.txt

## Starting the Embedding Service through Flask API use following command:
uvicorn main:app --reload --host 0.0.0.0 --port 8001 
## Start the Node js Orchestrator Service for Chatting:
npm install -g nodemon
nodemon index.js

## Start services using Docker Compose or can check these endpoints in Postman

docker-compose up --build

Embedding Service: http://0.0.0.0:8001

Orchestrator: http://127.0.0.1:3000

MongoDB Atlas: configured via MONGO_URI in .env or docker-compose

## API Specifications
Embedding Service

POST /embed

{
  "id": "doc_1",
  "text": "Your document text here."
}


Response:

{
  "id": "doc_1",
  "embedding": [0.1, 0.2, ...]
}


POST /bulk_embed

Accepts a list of documents for batch embedding and upsert.

Orchestrator Service

POST /chat

{
  "user_id": "user123",
  "query": "What is RAG?",
  "k": 5
}


Response:

{
  "answer": "Retrieval-Augmented Generation (RAG) combines...",
  "source_docs": ["doc_1", "doc_2"],
  "timing_ms": 123
}

‚ö° LoRA Adapter Inference

Load and apply a saved LoRA adapter:

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load base model and tokenizer
base_model = AutoModelForCausalLM.from_pretrained("distilgpt2")
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token
base_model.resize_token_embeddings(len(tokenizer))

# Load LoRA adapter
model = PeftModel.from_pretrained(base_model, "scripts/lora_distilgpt2", ignore_mismatched_sizes=True)

# Generate response
inputs = tokenizer("What is the capital of France?", return_tensors="pt", padding=True)
output_ids = model.generate(inputs.input_ids, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)
print(tokenizer.decode(output_ids[0], skip_special_tokens=True))

üß™ Demo Script

A short shell demo (demo_script.sh) illustrates:

Ingesting documents

Querying the RAG system

Loading and applying a LoRA adapter

## Run this Command in Terminal

chmod +x demo_script.sh
./demo_script.sh

‚öñÔ∏è Notes, Tradeoffs, and Scaling

Tradeoffs: Using LoRA adapters reduces memory usage and fine-tuning costs compared to full model fine-tuning.

Scaling: Docker Compose is suitable for local testing. For production, use Kubernetes or cloud services with GPU support.

Cost / Latency: Vector search in MongoDB Atlas is faster than CPU-based FAISS for small datasets. GPU inference significantly reduces latency for transformer-based models.

üìÇ File Structure
RAG_ASSESMENT/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ lora_inference.py
‚îÇ   ‚îî‚îÄ‚îÄ lora_distilgpt2/
‚îú‚îÄ‚îÄ embedding_service/
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ orchestrator/
‚îÇ   ‚îú‚îÄ‚îÄ server.js
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ demo_script.sh
‚îî‚îÄ‚îÄ README.md

üîó References

Hugging Face Transformers

PEFT / LoRA

MongoDB Atlas Vector Search


This README includes:

- Clean ASCII architecture diagram  
- Step-by-step local setup  
- API endpoints with request

## Short Cover Letter:

# Cover Note for RAG Assessment Project

## Assumptions
1. **Vector Store:** Using MongoDB Atlas with vector search enabled. FAISS is provided as a local alternative but not used in production.
2. **Embedding Service:** Python FastAPI service generates embeddings via OpenAI or local transformer models.
3. **Orchestrator:** Node.js/Express service handles query orchestration, retrieves top-k relevant documents, and calls an LLM for answers.
4. **LoRA Adapter:** DistilGPT2 small model is used for fine-tuning demonstrations. The adapter is applied locally; no hosted inference endpoint is provided.
5. **Document Chunking:** Basic chunking implemented with configurable size and overlap. Large-scale optimizations not included.

## Shortcuts Taken
1. **Training Data:** Toy examples are used for LoRA/PEFT fine-tuning instead of large-scale datasets.
2. **Local Testing:** Services are mostly tested on `localhost`; container orchestration tested with Docker Compose on a single machine.
3. **Security:** Secrets such as API keys are included in `.env` for local testing but excluded from Git via `.gitignore`. GitHub push protection blocked earlier pushes containing secrets.
4. **Docker Images:** Lightweight images created, volumes mapped locally. No cloud deployment automation.

## Known Incomplete / Limitations
1. **Scaling & Latency:** The pipeline is not optimized for high-volume queries or concurrent users.
2. **LoRA Adapter Integration:** Only CPU-based inference is demonstrated. GPU optimization is not included.
3. **Error Handling:** Minimal error handling in orchestrator and embedding service.
4. **Persistence of Large Files:** Model weights and adapters are excluded from the Git repo to comply with GitHub size limits; users must download or generate them locally.

## Notes
- All services can be run locally using Docker Compose.
- `.env` file must be created with valid keys for OpenAI or Hugging Face access.
- Curl or Postman scripts provided for demo of:
  1. Document ingestion
  2. Querying with context-aware answers
  3. Loading LoRA adapter for inference
- The project demonstrates a proof-of-concept for Retrieval-Augmented Generation (RAG) using lightweight components.

---
## DEMO VIDEO:
[![Demo Video](https://img.youtube.com/vi/bVXwBsOHdpM/0.jpg)](https://www.youtube.com/watch?v=bVXwBsOHdpM)
**Author:** Surbhi Sharma  
**Date:** September 2025
